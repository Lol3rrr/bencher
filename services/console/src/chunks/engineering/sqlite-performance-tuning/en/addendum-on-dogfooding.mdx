import QueryTimeBefore from "../query-time-before.mdx";
import QueryTimeAfter from "../query-time-after.mdx";

## Addendum on Dogfooding

I'm already [dogfooding Bencher with Bencher][bencher perf],
but all of the existing [benchmark harness adapters][adapters] are for micro-benchmarking harnesses.
Most HTTP harnesses are really load testing harnesses,
and [load testing is different than benchmarking][continuous benchmarking load testing].
Further, I'm not looking to expand Bencher into load testing anytime soon.
That is a very different use case that would require very different design considerations,
like that time series database for instance.
Even if I did have load testing in place,
I would really need to be running against a fresh pull of production data for this to have been caught.
The performance differences for these changes were negligible with my test database.

<details>
    <summary>Click to view test database benchmark results</summary>
    <br />

    Before:

    <QueryTimeBefore />

    After indexes and materialized view:

    <QueryTimeAfter />
</details>

<br />

All of this leads me to believe that I should create a micro-benchmark
that runs against the Perf API endpoint and dogfood the results with Bencher.
This will require a sizable test database
to make sure that these sort of performance regressions get caught in CI.
I have [created a tracking issue][github issue 367] for this work, if you would like to follow along.

This has all got me thinking though:
What if you could do [snapshot testing][snapshot testing] of your SQL database query plan?
That is, you could compare your current vs candidate SQL database query plans.
SQL query plan testing would sort of be like instruction count based benchmarking for databases.
The query plan helps to indicate that there may be an issue with the runtime performance,
without having to actually benchmark the database query.
I have [created a tracking issue][github issue 368] for this as well.
Please, feel free to add a comment with thoughts or any prior art that you are aware of!

[github issue 367]: https://github.com/bencherdev/bencher/issues/367
[github issue 368]: https://github.com/bencherdev/bencher/issues/368

[bencher perf]: /perf/bencher
[adapters]: /docs/explanation/adapters/
[continuous benchmarking load testing]: /docs/explanation/continuous-benchmarking/#continuous-benchmarking-vs-continuous-load-testing
[snapshot testing]: https://en.wikipedia.org/wiki/Software_testing#Output_comparison_testing
